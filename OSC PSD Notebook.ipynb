{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8ad289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.osc_model import calc_auc, open_pickle, turbiscan_resampling, get_raw_files, open_pickle\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2d9afc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC data using backscattering data and time 0 - 1 hr using 500 estimators for all ET models, clay and silt mse to guide\n",
    "# the MC, and 300 train-test splits. \n",
    "\n",
    "# Iif retraining the model, ensure the column names of your PSD input file has the same as the example\n",
    "\n",
    "training = True\n",
    "output_prefix = 'trial'\n",
    "iterations = 3\n",
    "\n",
    "if training:\n",
    "    \n",
    "    # Takes as input a file with the particle size distribution values (clay, silt, sand) for each training sample\n",
    "    psd_df = pd.read_excel('training_pipette_psd.xlsx') # Replace this file with your own PSD data\n",
    "    psd_df.set_index('sampleid', inplace = True)\n",
    "    \n",
    "    #Grab and process the turbiscan output files, note the turbiscan output files must be *.csv files\n",
    "    back_data, trans_data = get_raw_files(training)\n",
    "    \n",
    "    # Calculate the area under the curve (AUC) feature for each scan\n",
    "    back_df = back_data.iloc[:,:-3]\n",
    "    trans_df = trans_data.iloc[:,:-3]\n",
    "    auc_back_df = calc_auc(back_df)\n",
    "    auc_trans_df = calc_auc(trans_df)\n",
    "    \n",
    "    # Set up the backscattering and transmission data inputs to use only the first 6 scans and add the PSD data\n",
    "    # (i.e., the first 10 minutes if using a 2 minute scan)\n",
    "    df = pd.merge(pd.merge(auc_back_df.iloc[:,:6], auc_trans_df.iloc[:,:6], on = 'sampleid'), psd_df, on = 'sampleid')\n",
    "    df.columns = df.columns.astype(str)\n",
    "\n",
    "    # Set up the X and Y data\n",
    "    y = df['clay']\n",
    "    X = df.iloc[:,:-3]\n",
    "\n",
    "    # Run the resampling for iterations number of train-test-validate splits\n",
    "    run_name = '' # Set this to whatever name you wish to call the output pickle file\n",
    "    turbiscan_resampling(output_prefix, df, X, y, iters = iterations)\n",
    "    \n",
    "else:\n",
    "    # Grab and process the turbiscan output files, note the turbiscan output files must be *.csv files\n",
    "    back_df, trans_df = get_raw_files(training)\n",
    "    \n",
    "    # Calculate the area under the curve (AUC) feature for each scan\n",
    "    auc_back_df = calc_auc(back_df)\n",
    "    auc_trans_df = calc_auc(trans_df)\n",
    "    \n",
    "    # Set up the backscattering and transmission data inputs to use only the first 6 scans\n",
    "    # (i.e., the first 10 minutes if using a 2 minute scan)\n",
    "    merged_df = pd.merge(auc_back_df.iloc[:,:6], auc_trans_df.iloc[:,:6], right_index = True, left_index = True)\n",
    "\n",
    "    # Open the pickled models\n",
    "    clay_model = open_pickle('turbiscan_model/clay_model.pkl')\n",
    "    silt_model = open_pickle('turbiscan_model/silt_model.pkl')\n",
    "\n",
    "    #Calculate the clay and silt components of the PSD using the backscattering and transmission inputs\n",
    "    psd_results=pd.DataFrame([clay_model.predict(merged_df), silt_model.predict(merged_df)]).T\n",
    "    psd_results.index = merged_df.index\n",
    "    psd_results.columns = ['clay', 'silt']\n",
    "    \n",
    "    # Calculate the sand component using the formula 100% - (clay + silt)\n",
    "    psd_results['sand'] = 100-psd_results.sum(axis=1)\n",
    "\n",
    "# Unpack the results\n",
    "if training:\n",
    "    # Unpack the pickle file with all the run information\n",
    "    results = open_pickle(output_prefix+\"_OSC_results.pkl\")\n",
    "    error_distributions, final_training_test_set, final_validate_set, best_clay_model, best_silt_model, best_sand_model = results\n",
    "else:\n",
    "    # Print the predicted PSD results\n",
    "    print(psd_results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d14f684-63a2-4c43-a5c6-51eab647d8ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
