{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8ad289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.osc_model import calc_auc, open_pickle, turbiscan_resampling, read_turbiscan, open_pickle, clear_cache, get_trans_back\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2d9afc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    clay       silt       sand\n",
      "sampleid                                      \n",
      "AnzaBorrego4A   9.024796   6.849228  84.125976\n",
      "AnzaBorrego4B  12.778165   9.165200  78.056635\n",
      "AnzaBorrego4C  12.590780   9.690164  77.719057\n",
      "Backbone1A      6.099177  11.822811  82.078012\n",
      "Backbone1B      6.099177  11.822811  82.078012\n",
      "...                  ...        ...        ...\n",
      "Yuba7B          7.659624   7.380293  84.960083\n",
      "Yuba7C          6.099177  11.822811  82.078012\n",
      "Zippel7A        6.099177  11.822811  82.078012\n",
      "Zippel7B        6.099177  11.822811  82.078012\n",
      "Zippel7C        6.099177  11.822811  82.078012\n",
      "\n",
      "[123 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# AUC data using backscattering data and time 0 - 1 hr using 500 estimators for all ET models, clay and silt mse to guide\n",
    "# the MC, and 300 train-test splits. \n",
    "\n",
    "# Iif retraining the model, ensure the column names of your PSD input file has the same as the example\n",
    "data_path = 'run files'\n",
    "training = False\n",
    "output_prefix = 'trial'\n",
    "iterations = 300\n",
    "\n",
    "# Clear the cache directory and write your data to the cache *** Comment these lines out if you have already written your data to the cache ***\n",
    "clear_cache()\n",
    "read_turbiscan(data_path)\n",
    "\n",
    "# Read in the backscattering and transmission data from cache directory and calculate the AUC\n",
    "back_df, trans_df = get_trans_back()\n",
    "\n",
    "back_df.fillna(0, inplace = True)\n",
    "auc_back_df = calc_auc(back_df)\n",
    "\n",
    "trans_df.fillna(0, inplace = True)\n",
    "auc_trans_df = calc_auc(trans_df)\n",
    "\n",
    "if training:\n",
    "\n",
    "    # Takes as input a file with the particle size distribution values (clay, silt, sand) for each training sample\n",
    "    psd_df = pd.read_excel('training_pipette_psd.xlsx') # Replace this file with your own PSD data\n",
    "    psd_df.set_index('sampleid', inplace = True)\n",
    "       \n",
    "    # Set up the backscattering and transmission data inputs to use only the first 6 scans and add the PSD data\n",
    "    # (i.e., the first 10 minutes if using a 2 minute scan)\n",
    "    df = pd.merge(pd.merge(auc_back_df.iloc[:,0:6], auc_trans_df.iloc[:,0:6], right_index = True, left_index = True), psd_df, right_index = True, left_index = True)\n",
    "    df.columns = df.columns.astype(str)\n",
    "\n",
    "    # Set up the X and Y data\n",
    "    y = df['clay']\n",
    "    X = df.iloc[:,:-3]\n",
    "\n",
    "    # Run the resampling for iterations number of train-test-validate splits\n",
    "    turbiscan_resampling(output_prefix, df, X, y, iters = iterations)\n",
    "\n",
    "    # Unpack the pickle file with all the run information\n",
    "    results = open_pickle(output_prefix+\"_OSC_results.pkl\")\n",
    "    error_distributions, final_training_test_set, final_validate_set, best_clay_model, best_silt_model, best_sand_model = results\n",
    "    \n",
    "else:\n",
    "    \n",
    "    #  Set up the backscattering and transmission data inputs to use only the first 6 scans\n",
    "    # (i.e., the first 10 minutes if using a 2 minute scan)\n",
    "    merged_df = pd.merge(auc_back_df.iloc[:,0:6], auc_trans_df.iloc[:,0:6], right_index = True, left_index = True)\n",
    "\n",
    "    # Get the pickled models and read in the clay and silt models\n",
    "    directory = 'turbiscan_model'\n",
    "    for fid in os.listdir(directory):\n",
    "        full_fid = os.path.join(os.getcwd(),directory, fid)\n",
    "        if 'clay' in fid:\n",
    "            clay_model = open_pickle(full_fid)\n",
    "        elif 'silt' in fid:\n",
    "            silt_model = open_pickle(full_fid)\n",
    "\n",
    "    # Predict silt and clay using the pickled models and use these 2 to calculate sand\n",
    "    results=pd.DataFrame([clay_model.predict(merged_df), silt_model.predict(merged_df)]).T\n",
    "    results.index = merged_df.index\n",
    "    results.columns = ['clay', 'silt']\n",
    "    results['sand'] = 100-results.sum(axis=1)\n",
    "\n",
    "    # Write the results to a csv file\n",
    "    results.to_csv(output_prefix+'_predicted_psd.csv')\n",
    "\n",
    "    print(results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73381e4a-3cdd-4846-b280-5d98e8361cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
